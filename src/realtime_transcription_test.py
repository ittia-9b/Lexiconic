#!/usr/bin/env python3
"""
OpenAI Realtime Transcription Test Script

This script demonstrates how to use OpenAI's Realtime API for live audio transcription.
It captures audio from your microphone and sends it to OpenAI for real-time transcription.

Requirements:
- OpenAI API key
- pyaudio for audio capture
- websockets for real-time communication
"""

import asyncio
import json
import base64
import os
import sys
import threading
import time
import queue
from typing import Optional
import shutil
import asyncio.subprocess

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("Missing python-dotenv. Install with: pip install python-dotenv")
    # Continue if not present, fallback to normal env


try:
    import aiohttp
except ImportError:
    print("Missing required packages. Install with:")
    print("pip install aiohttp openai")
    sys.exit(1)

# Make pyaudio optional so file-streaming tests can run without it
try:
    import pyaudio
except Exception:
    pyaudio = None

# Audio configuration
SAMPLE_RATE = 24000  # OpenAI Realtime API expects 24kHz
CHUNK_SIZE = 1024
CHANNELS = 1
FORMAT = pyaudio.paInt16 if pyaudio else None

class RealtimeTranscriber:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.websocket = None
        self.audio_stream = None
        self.pyaudio_instance = None
        self.is_recording = False
        self.transcription_buffer = []
        self.audio_queue = queue.Queue()
        self.loop = None
        
    async def connect(self):
        """Connect to OpenAI Realtime API"""
        uri = "wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-10-01"
        
        try:
            # Use aiohttp for WebSocket connection with proper headers
            self.session = aiohttp.ClientSession()
            self.websocket = await self.session.ws_connect(
                uri,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "OpenAI-Beta": "realtime=v1"
                }
            )
            print("âœ… Connected to OpenAI Realtime API")
            
            # Send session configuration
            session_config = {
                "type": "session.update",
                "session": {
                    "modalities": ["text", "audio"],
                    "instructions": "You are a helpful assistant that transcribes audio in real-time.",
                    "voice": "alloy",
                    "input_audio_format": "pcm16",
                    "output_audio_format": "pcm16",
                    "input_audio_transcription": {
                        "model": "whisper-1"
                    },
                    "turn_detection": {
                        "type": "server_vad",
                        "threshold": 0.7,  # Increased threshold for less sensitivity
                        "prefix_padding_ms": 500,  # Increased padding before speech
                        "silence_duration_ms": 500  # Increased silence duration to end speech
                    }
                }
            }
            
            await self.websocket.send_str(json.dumps(session_config))
            print("ğŸ“ Session configured for transcription")
            
        except Exception as e:
            print(f"âŒ Failed to connect: {e}")
            raise
    
    def setup_audio(self):
        """Initialize audio capture"""
        if pyaudio is None:
            raise RuntimeError("pyaudio not available; microphone capture disabled. Install pyaudio to use mic.")
        self.pyaudio_instance = pyaudio.PyAudio()
        
        try:
            self.audio_stream = self.pyaudio_instance.open(
                format=FORMAT,
                channels=CHANNELS,
                rate=SAMPLE_RATE,
                input=True,
                frames_per_buffer=CHUNK_SIZE
            )
            print("ğŸ¤ Audio capture initialized")
        except Exception as e:
            print(f"âŒ Failed to initialize audio: {e}")
            raise
    
    async def _audio_capture_loop(self):
        """Continuously capture audio and queue it for sending"""
        while self.is_recording:
            try:
                if self.audio_stream:
                    # Read audio data
                    audio_data = self.audio_stream.read(CHUNK_SIZE, exception_on_overflow=False)
                    
                    if self.websocket and not self.websocket.closed:
                        # Create audio event
                        audio_event = {
                            "type": "input_audio_buffer.append",
                            "audio": base64.b64encode(audio_data).decode('utf-8')
                        }
                        
                        # Send audio data
                        await self._send_audio_data(audio_event)
                
                # Small delay to prevent overwhelming the API
                await asyncio.sleep(0.01)
                
            except Exception as e:
                print(f"âš ï¸ Error in audio capture: {e}")
                await asyncio.sleep(0.1)
    
    async def _send_audio_data(self, audio_event):
        """Send audio data to websocket"""
        try:
            if self.websocket and not self.websocket.closed:
                await self.websocket.send_str(json.dumps(audio_event))
        except Exception as e:
            print(f"âš ï¸ Error sending audio data: {e}")
    
    async def listen_for_responses(self):
        """Listen for responses from OpenAI"""
        try:
            async for msg in self.websocket:
                if msg.type == aiohttp.WSMsgType.TEXT:
                    try:
                        data = json.loads(msg.data)
                        await self._handle_response(data)
                    except json.JSONDecodeError:
                        print(f"âš ï¸ Invalid JSON received: {msg.data}")
                elif msg.type == aiohttp.WSMsgType.ERROR:
                    print(f"âŒ WebSocket error: {self.websocket.exception()}")
                    break
                elif msg.type == aiohttp.WSMsgType.CLOSE:
                    print("ğŸ”Œ Connection closed")
                    break
        except Exception as e:
            print(f"âŒ Error listening for responses: {e}")
    
    async def _handle_response(self, data):
        """Handle different types of responses from OpenAI"""
        event_type = data.get("type", "")
        
        if event_type == "session.created":
            print("ğŸ¯ Session created successfully")
        
        elif event_type == "input_audio_buffer.speech_started":
            print("ğŸ—£ï¸ Speech detected...")
        
        elif event_type == "input_audio_buffer.speech_stopped":
            print("ğŸ¤« Speech ended")
        
        elif event_type == "conversation.item.input_audio_transcription.completed":
            transcript = data.get("transcript", "")
            if transcript:
                print(f"ğŸ“ Transcription: {transcript}")
                self.transcription_buffer.append(transcript)
        
        elif event_type == "conversation.item.input_audio_transcription.failed":
            error = data.get("error", {})
            print(f"âŒ Transcription failed: {error}")
        
        elif event_type == "error":
            error = data.get("error", {})
            print(f"âŒ API Error: {error}")
        
        # Uncomment to see all events for debugging
        # else:
        #     print(f"ğŸ“¨ Event: {event_type}")
    
    async def start_recording(self):
        """Start audio recording"""
        self.is_recording = True
        print("ğŸ”´ Recording started - speak into your microphone!")
        
        # Start the audio capture loop
        asyncio.create_task(self._audio_capture_loop())
    
    def stop_recording(self):
        """Stop audio recording"""
        self.is_recording = False
        if self.audio_stream:
            self.audio_stream.stop_stream()
        print("â¹ï¸ Recording stopped")
    
    async def cleanup(self):
        """Clean up resources"""
        self.stop_recording()
        
        if self.websocket and not self.websocket.closed:
            await self.websocket.close()
        
        if hasattr(self, 'session') and self.session:
            await self.session.close()
        
        if self.audio_stream:
            self.audio_stream.close()
        
        if self.pyaudio_instance:
            self.pyaudio_instance.terminate()
        
        print("ğŸ§¹ Cleanup completed")
    
    async def stream_file(self, file_path: str, realtime_factor: float = 1.0, send_commit: bool = True):
        """Stream an audio file to the realtime API as if it were live microphone input.
        The file is decoded to 24kHz mono PCM16 using ffmpeg, chunked, and paced to real-time.
        
        Args:
            file_path: Path to the audio file to stream (e.g., .m4a).
            realtime_factor: 1.0 streams at real-time speed; >1.0 streams faster; <1.0 slower.
            send_commit: If True, sends an input_audio_buffer.commit at the end to flush.
        """
        if shutil.which("ffmpeg") is None:
            raise RuntimeError("ffmpeg is required to stream audio files. Install it, e.g., `brew install ffmpeg`.")
        
        try:
            proc = await asyncio.create_subprocess_exec(
                "ffmpeg", "-loglevel", "error", "-i", file_path,
                "-f", "s16le", "-acodec", "pcm_s16le", "-ac", "1", "-ar", str(SAMPLE_RATE),
                "pipe:1",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
        except Exception as e:
            raise RuntimeError(f"Failed to start ffmpeg for {file_path}: {e}")
        
        bytes_per_frame = 2  # 16-bit mono
        chunk_bytes = CHUNK_SIZE * bytes_per_frame
        seconds_per_chunk = CHUNK_SIZE / float(SAMPLE_RATE)
        
        while True:
            data = await proc.stdout.read(chunk_bytes)
            if not data:
                break
            audio_event = {
                "type": "input_audio_buffer.append",
                "audio": base64.b64encode(data).decode("utf-8")
            }
            await self._send_audio_data(audio_event)
            # Pace like real-time
            await asyncio.sleep(max(0.0, seconds_per_chunk / max(realtime_factor, 1e-6)))
        
        await proc.wait()
        
        if send_commit:
            try:
                await self.websocket.send_str(json.dumps({"type": "input_audio_buffer.commit"}))
            except Exception as e:
                print(f"âš ï¸ Error sending commit event: {e}")
    
    def get_transcriptions(self):
        """Get all transcriptions collected so far"""
        return self.transcription_buffer.copy()

async def main():
    """Main function to run the realtime transcription test"""
    # Get API key
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ Please set your OPENAI_API_KEY environment variable")
        print("   export OPENAI_API_KEY='your-api-key-here'")
        return
    
    transcriber = RealtimeTranscriber(api_key)
    
    try:
        # Setup
        print("ğŸš€ Starting OpenAI Realtime Transcription Test")
        print("=" * 50)
        
        await transcriber.connect()
        transcriber.setup_audio()
        
        # Start listening for responses in background
        response_task = asyncio.create_task(transcriber.listen_for_responses())
        
        # Start recording
        await transcriber.start_recording()
        
        print("\nğŸ’¡ Instructions:")
        print("- Speak into your microphone")
        print("- Press Enter to stop recording")
        print("- Type 'quit' to exit")
        print("=" * 50)
        
        # Wait for user input
        def wait_for_input():
            while True:
                user_input = input().strip().lower()
                if user_input == 'quit':
                    return 'quit'
                elif user_input == '':
                    return 'stop'
        
        # Run input waiting in a thread
        input_thread = threading.Thread(target=wait_for_input)
        input_thread.daemon = True
        input_thread.start()
        
        # Keep running until user stops
        while input_thread.is_alive():
            await asyncio.sleep(0.1)
        
        # Stop recording
        transcriber.stop_recording()
        
        # Wait a moment for final transcriptions
        await asyncio.sleep(2)
        
        # Show results
        transcriptions = transcriber.get_transcriptions()
        if transcriptions:
            print("\nğŸ“‹ Final Transcriptions:")
            print("=" * 50)
            for i, transcript in enumerate(transcriptions, 1):
                print(f"{i}. {transcript}")
        else:
            print("\nğŸ“­ No transcriptions captured")
        
        # Cancel response task
        response_task.cancel()
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Interrupted by user")
    except Exception as e:
        print(f"âŒ Error: {e}")
    finally:
        await transcriber.cleanup()
        print("ğŸ‘‹ Test completed!")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Goodbye!")
